
\section{Test Distributions}

The models corresponding to the distributions, that are introduced in the following, will be explored in the experiments in the results section.
This is just a small selection, that will not cover many interesting questions or potential test cases, 
so the reader is invited to customise the framework, introduced later in this report, by adding a new model they consider interesting.
In the following, let $X: \Omega \to \mathbb{R}$ or $\bm{X}: \Omega \to \mathbb{R}^d$ be a random variable or a random vector respectively,
with the respective distribution.


\subsection{Multivariate Normal Distribution}

The multivariate normal distribution is a great place to start as it is ubiquitous, easy to handle and serves as a base component for many complex models and algorithms.
For mean $\bm{\mu} \in \mathbb{R}^d$ and covariance $\Sigma \in \mathbb{R}^{d \times d}$ we say $X$ follows a multivariate Gaussian 
or normal distribution $X \sim \mathcal{N}(\bm{\mu}, \Sigma)$, if its probability density function is given by:
%\textbf{Probability Density Function (pdf):}
\[
	f_X(\bm{x} | \bm{\mu}, \Sigma) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} \exp\left(-\frac{1}{2} (\bm{x} - \bm{\mu})^T \Sigma^{-1} (\bm{x} - \bm{\mu})\right)
\]

From this density one can obtain several components used later in sampling and estimation:
\textbf{Kernel:}
\[
	K_X (\bm{x}) = \exp \left( -\frac{1}{2} (\bm{x} - \bm{\mu})^T \Sigma^{-1} (\bm{x} - \bm{\mu}) \right)
\]

\textbf{Energy Function:}
\[
\begin{aligned}
	U(\bm{x}) &= - \log( K_X (\bm{x}) ) \\
	&= - \log( \exp \left( -\frac{1}{2} (\bm{x} - \bm{\mu})^T \Sigma^{-1} (\bm{x} - \bm{\mu}) \right) ) \\
	&= \left( \frac{1}{2} (\bm{x} - \bm{\mu})^T \Sigma^{-1} (\bm{x} - \bm{\mu}) \right)
\end{aligned}
\]

\textbf{Gradient of Energy Function with respect to $\bm{x}$:}
\[
\begin{aligned}
	\nabla U(\bm{x}) &= \nabla \left( \frac{1}{2} (\bm{x} - \bm{\mu})^T \Sigma^{-1} (\bm{x} - \bm{\mu}) \right) \\
	&= \Sigma^{-1} (\bm{x} - \bm{\mu}) \\
\end{aligned}
\]

\textbf{Gradient of Energy Function with respect to $\bm{\mu}$:}
\[
\begin{aligned}
	\nabla_{\bm{\mu}} U(\bm{x}) &= \nabla_{\bm{\mu}} \left( \frac{1}{2} (\bm{x} - \bm{\mu})^T \Sigma^{-1} (\bm{x} - \bm{\mu}) \right) \\
	&= \Sigma^{-1} (\bm{\mu} - \bm{x}) \\
\end{aligned}
\]

The partial derivatives of the energy functional with respect to $\Sigma_{i,j}$ however are not as easily obtainable analytically,
so the gradient with respect to $\Sigma$ is obtained instead with automatic differentiation, that is leveraged by the employed framework.


\subsection{Gaussian Mixture Distribution (GMD)}

Gaussian mixture distributions make for simple but sufficiently complex distribution for our parameter estimation testing process.
A GMD is a random vector  $\bm{X} : \Omega \to \mathbb{R}^d$, whose density is the weighted sum of the densities $f_i : \mathbb{R}^d \to \mathbb{R}$, 
of $m$ Gaussian random vectors $\bm{Z}_i \sim \mathcal{N}(\bm{\mu}_i , \Sigma_i )$ with corresponding weights $w_i \in (0, 1)$ such that $\sum_{i = 1}^m w_i$.
The density is given by
\[
	p_{\bm{\theta}} (\bm{x}) := \sum_{i = 1}^m w_i f_i(\bm{x})
\]

where the weights, means and covariances are aggregated in the parameter $\bm{\theta}$ for convenience.
For each $i \in [m]$ the density $f_i$ can be split in two parts, namely the kernel, which is denoted as $K_i$, and the normalisation constant $Z_i$, i.e.
\[
	f_i (\bm{x}) 
	= \underbrace{ \frac{1}{\sqrt{(2\pi)^{d} |\Sigma_i |}} }_{=: Z_i} 
	\underbrace{ \exp \left( - \frac{1}{2} (\bm{x} - \bm{\mu}_i)^T \Sigma_i^{-1} (\bm{x} - \bm{\mu_i}) \right) }_{=: K_i (\bm{x})}
\]

and with these one can rewrite the density as
\[
\begin{aligned}
	p_{\bm{\theta}} (\bm{x}) 
	&= \sum_{i = 1}^m w_i \frac{K_i(\bm{x})}{Z_i} \\
	&= \sum_{i = 1}^m \frac{ w_i (\prod_{j \neq i} Z_j ) K_i(\bm{x}) }{ \prod_{j = 1}^m Z_j } \\
	&= \frac{ \sum_{i = 1}^m w_i (\prod_{j \neq i} Z_j ) K_i(\bm{x}) }{ \prod_{j = 1}^m Z_j } \\
\end{aligned}
\]

By defining 
\[
	K_{\bm{\theta}} (\bm{x}) = \sum_{i = 1}^m w_i (\prod_{j \neq i} Z_j ) K_i(\bm{x})
\]

we obtain this expression for the energy $U_{\bm{\theta}}$:
\[
\begin{aligned}
	U_{\bm{\theta}} (\bm{x}) 
	&= - \log(K_{\bm{\theta}} (\bm{x}) )  \\
	&= \log( \frac{ 1 }{ \sum_{i = 1}^m w_i (\prod_{j \neq i} Z_j ) K_i(\bm{x})} )\\
\end{aligned}
\]

Here neither the gradient of $U_{\bm{\theta}}$ w.r.t. the inputs $\bm{x}$ nor w.r.t. to the parameters $\bm{\theta}$ is simple to calculate,
and hence the framework leverages the automatic differentiation capacity of \texttt{pytorch} for both of these.


\subsection{Univariate Polynomial Energy}

A polynomial can serve as an energy functional for an EBM and here, instead of deriving the energy from the density, the distribution is defined via the energy directly.
This is also the more common approach in practice, where typically an energy is defined to create a distribution.
Let $m \in \mathbb{N}$ and $\forall i \in [m]: w_i \in \mathbb{R}$ and for $x \in \mathbb{R}$ define the energy functional as
\[
	U_{\bm{w}} (x) := \sum_{i = 1}^m w_i x^i .
\]

Then the density is given by
\[
	p_{\bm{w}} (x) := \frac{ \exp( - \sum_{i = 1}^m w_i x^i ) }{ Z_{\bm{w}} }
\]

where $Z_{\bm{w}}$ can, in general, only be approximated numerically.
In this case it is easy to derive the analytical gradients of $U_{\bm{w}}$ directly.

Gradient of $U_{\bm{w}}$ with respect to $x$:
\[
	\frac{ \dif }{ \dif x} U_{\bm{w}} (x) := \sum_{i = 1}^{m} i w_i x^{i -1}
\]

Gradient of $U_{\bm{w}}$ with respect to $\bm{w}$:
\[
	\nabla_{\bm{w}} U_{\bm{w}} (x) := \sum_{i = 1}^{m} \bm{e}_i x^i
\]

where $\bm{e}_i = (\delta_{i, j})_{j=1}^m$ is the $i$th unit vector in $\mathbb{R}^m$.









\begin{comment}

\subsection{The Exponential Family}

A parameterised exponential family is a set of probability distributions whose probability density function $p$ can be expressed in the form

\[
	p(\bm{x} | \bm{\theta}) = h(\bm{x}) \exp \left[ \eta(\bm{\theta})^T  \phi(\bm{x}) - A(\eta(\bm{\theta})) \right]
\]

where

\begin{enumerate}
	\item $\bm{\theta}$ is called \textit{natural} or \textit{canonical} parameters
	\item $\phi(\bm{x})$ is the vector of \textit{sufficient statistics}
	\item $\eta$ maps the parameters $\bm{\theta}$ to the canonical parameters
	\item $A$ is the \textit{log-partition function} or \textit{cumulant function}
\end{enumerate}

We can rearrange the form of the exponential family to obtain an EBM version:
\[
\begin{aligned}
	p(\bm{x} | \bm{\theta}) 
	&= h(\bm{x}) \exp \left[ \eta(\bm{\theta})^T  \phi(\bm{x}) - A(\eta(\bm{\theta})) \right] \\
	&= \frac{1}{ \exp \left( A(\eta(\bm{\theta}) ) \right) } \exp \left[ \eta(\bm{\theta})^T  \phi(\bm{x}) + \log(h(\bm{x}))  \right] \\
\end{aligned}
\]

and define the energy function of such a model as
\[
	U_{\bm{\theta}}(\bm{x}) = - \eta(\bm{\theta})^T  \phi(\bm{x}) - \log(h(\bm{x})).
\]

The gradient w.r.t. $\bm{x}$ is then given by
\[
	\nabla_{\bm{x}} U_{\bm{\theta}}(\bm{x}) = - \eta(\bm{\theta})^T  J_{\phi} (\bm{x})  - \frac{\nabla_{\bm{x}} h(\bm{x})}{h(\bm{x})}
\]
where $J_{\phi}$ is the Jacobian matrix of $\phi$ with entries $(J_{\phi} (\bm{x}))_{i, j} = \frac{\del}{\del x_j} \phi(\bm{x})_i $ 
corresponding to the partial derivatives of the components of $\phi(\bm{x})$.
The gradient w.r.t. the parameter similarly becomes
\[
	\nabla_{\bm{\theta}} U_{\bm{\theta}}(\bm{x}) = - \phi(\bm{x})^T  J_{\eta} (\bm{\theta}) 
\]

%--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

where the weights must satisfy $\sum_i w_i = 1$. If we aggregate the weights into a vector $\bm{w} \in (0, 1)^m$ 
and the component density functions into a vector $\bm{f}$, we can write the density of $\bm{X}$ as:
\[
	p_{\bm{w}} (\bm{x}) = \bm{w}^T \bm{f}(\bm{x})
\]


While we don't have a simple expression for the energy functional we can still obtain the gradient with respect to $\bm{x}$ as we have shown before by calculating
\[
\begin{aligned}
	\nabla_{\bm{x}} \log p_{\bm{w}} (\bm{x}) 
	&= \nabla_{\bm{x}} \log ( \bm{w}^T \bm{f}(\bm{x}) ) \\
	&= \frac{\nabla_{\bm{x}} ( \bm{w}^T \bm{f}(\bm{x}) )}{ ( \bm{w}^T \bm{f}(\bm{x}) ) } \\
	&= \frac{ \sum_{i = 1}^m w_i \nabla_{\bm{x}} f_i (\bm{x}) }{ \bm{w}^T \bm{f}(\bm{x}) } \\
	&= \frac{ \sum_{i = 1}^m w_i f_i (\bm{x}) \Sigma_i^{-1} (\bm{x} - \bm{\mu}_i) }{ ( \bm{w}^T \bm{f}(\bm{x}) ) } \\
\end{aligned}
\]

%--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

\subsection{Gaussian Distributions}


\textbf{Univariate Normal Distribution:}

For mean $\mu \in \mathbb{R}$ and standard deviation $\sigma \in \mathbb{R}$ we say $X$ follows a univariate Gaussian
or normal distribution $X \sim \mathcal{N}(\mu, \sigma^2)$, if its probability density function is given by:

\[
	f_X (x) = \frac{1}{\sigma \sqrt{2\pi}} \exp \left( -\frac{(x - \mu)^2}{2\sigma^2} \right)
\]

It then has the following properties

\textbf{Kernel:}
\[
	K_X(x) = \exp \left( -\frac{(x - \mu)^2}{2\sigma^2} \right)
\]

\textbf{Mean:}
\[
	\text{E}[X] = \mu
\]

\textbf{Variance:}
\[
	\text{Var}[X] = \sigma^2
\]

\textbf{Standard Deviation:}
\[
	\text{SD}[X] = \sigma
\]

\textbf{Cumulative Distribution Function (CDF):}
\[
	\Phi(x) = \frac{1}{2} \left[1 + \text{erf}\left(\frac{x - \mu}{\sigma \sqrt{2}}\right)\right]
\]

where \(\text{erf}\) is the error function.

\textbf{Energy Function:}
\[
	U(x) = - \log( K_X(x) ) = - \log( \exp \left( -\frac{(x - \mu)^2}{2\sigma^2} \right) ) = \frac{(x - \mu)^2}{2\sigma^2}
\]

\textbf{Gradient of Energy Function:}
\[
	\frac{\dif U(x)}{\dif x} = \frac{x - \mu}{\sigma^2}
\]



\textbf{Multivariate Normal Distribution:}

For mean $\bm{\mu} \in \mathbb{R}^d$ and standard deviation $\Sigma \in \mathbb{R}^{d \times d}$ we say $X$ follows a multivariate Gaussian 
or normal distribution $X \sim \mathcal{N}(\bm{\mu}, \Sigma)$, if its probability density function is given by:
\textbf{Probability Density Function (pdf):}
\[
	f_X(\bm{x} | \bm{\mu}, \Sigma) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} \exp\left(-\frac{1}{2} (\bm{x} - \bm{\mu})^T \Sigma^{-1} (\bm{x} - \bm{\mu})\right)
\]

\textbf{Kernel:}
\[
	K_X (\bm{x}) = \exp \left( -\frac{1}{2} (\bm{x} - \bm{\mu})^T \Sigma^{-1} (\bm{x} - \bm{\mu}) \right)
\]

\textbf{Mean Vector (\(\bm{\mu}\)):}
\[
\text{E}[\bm{X}] = \bm{\mu}
\]

\textbf{Covariance Matrix (\(\Sigma\)):}
\[
\text{Cov}[\bm{X}] = \Sigma
\]

\textbf{Characteristic Function:}
\[
\phi_{\bm{X}}(\bm{t}) = \exp\left(i \bm{t}^T \bm{\mu} - \frac{1}{2} \bm{t}^T \Sigma \bm{t}\right)
\]

\textbf{Energy Function:}
\[
\begin{aligned}
	U(\bm{x}) &= - \log( K_X (\bm{x}) ) \\
	&= - \log( \exp \left( -\frac{1}{2} (\bm{x} - \bm{\mu})^T \Sigma^{-1} (\bm{x} - \bm{\mu}) \right) ) \\
	&= \left( \frac{1}{2} (\bm{x} - \bm{\mu})^T \Sigma^{-1} (\bm{x} - \bm{\mu}) \right)
\end{aligned}
\]

\textbf{Gradient of Energy Function:}
\[
\begin{aligned}
	\nabla U(\bm{x}) &= \nabla \left( \frac{1}{2} (\bm{x} - \bm{\mu})^T \Sigma^{-1} (\bm{x} - \bm{\mu}) \right) \\
	&= \Sigma^{-1} (\bm{x} - \bm{\mu}) \\
\end{aligned}
\]

%--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


\subsection{Gamma Distribution}

For shape parameter $\alpha \in \mathbb{R}_+$ and rate parameter $\beta \in \mathbb{R}_+$ we say $X$ follows a Gamma distribution $X \sim \Gamma(\alpha, \beta)$, 
if its probability density function is given by:

\[
	f_X(x) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} \exp( -\beta x)
\]

where $\Gamma$ is the gamma function.

\textbf{Kernel:}
\[
	K_X(x) = x^{\alpha-1} \exp( -\beta x)
\]

\textbf{Mean:}
\[
	\text{E}[X] = \frac{\alpha}{\beta}
\]

\textbf{Variance:}
\[
	\text{Var}[X] = \frac{\alpha}{\beta^2}
\]

\textbf{Energy Function:}
\[
	U(x) = - \log( K_X(x) ) = - \log( x^{\alpha-1} \exp( -\beta x) ) = \beta x - (\alpha - 1) \log x 
\]

\textbf{Gradient of Energy Function:}
\[
	\frac{\dif U(x)}{\dif x} = \frac{ \dif }{ \dif x } \beta x -  \frac{ \dif }{ \dif x } (\alpha - 1) \log x = \beta - \frac{\alpha - 1}{x} 
\]

%--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------



\subsection{Beta Distribution}

For shape parameters $\alpha, \beta \in \mathbb{R}_+$ we say $X$ follows a Beta distribution $X \sim \text{Beta}(\alpha, \beta)$, 
if its probability density function for $0 \leq x \leq 1$ is given by:

\[
	f_X (x) = \frac{\Gamma(\alpha)\Gamma(\alpha)}{\Gamma(\alpha + \beta)} x^{\alpha-1}(1-x)^{\beta-1}
\]

where $\Gamma$ is the gamma function.

\textbf{Kernel:}
\[
	K_X(x) = x^{\alpha-1}(1-x)^{\beta-1}
\]

\textbf{Mean:}
\[
\text{E}[X] = \frac{\alpha}{\alpha + \beta}
\]

\textbf{Variance:}
\[
\text{Var}[X] = \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}
\]

\textbf{Energy Function:}
\[
	U(x) = - \log( K_X(x) ) = - \log( x^{\alpha - 1}(1-x)^{\beta - 1}) = - (\alpha - 1) \log x  - (\beta - 1) \log (1 - x)
\]

\textbf{Gradient of Energy Function:}
\[
	\frac{\dif U(x)}{\dif x} = - \frac{ \dif }{ \dif x } (\beta - 1) \log (1 - x) -  \frac{ \dif }{ \dif x } (\alpha - 1) \log x = \frac{\beta - 1}{1 - x}  - \frac{\alpha - 1}{x} 
\]

\[
\begin{aligned}
	\frac{\dif U(x)}{\dif x} :
	\begin{cases}
		\geq 0 \quad &\text{if} \quad x < 0, \, \beta \geq 1, \, \alpha \geq 1 \\
		\geq 0 \quad &\text{if}  \quad x \in (0,1], \, \beta \geq 1, \, \alpha \geq 1 \\
		\leq 0 \quad &\text{if}  \quad x > 1, \, \beta \geq 1, \, \alpha \geq 1 \\
	\end{cases}
\end{aligned}
\]


%--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


2. **Poisson Distribution:**
   \[ P(X = k; \lambda) = \frac{e^{-\lambda} \lambda^k}{k!} \]

3. **Exponential Distribution:**
   \[ f(x; \lambda) = \lambda e^{-\lambda x} \]

4. **Gamma Distribution:**
   \[ f(x; \alpha, \beta) = \frac{\beta^\alpha x^{\alpha-1} e^{-\beta x}}{\Gamma(\alpha)} \]

5. **Beta Distribution:**
   \[ f(x; \alpha, \beta) = \frac{x^{\alpha-1} (1-x)^{\beta-1}}{B(\alpha, \beta)} \]

6. **Bernoulli Distribution:**
   \[ P(X = k; p) = p^k (1-p)^{1-k} \text{ for } k \in \{0, 1\} \]

7. **Geometric Distribution:**
   \[ P(X = k; p) = (1-p)^{k-1} p \text{ for } k \in \{1, 2, \ldots\} \]

8. **Multinomial Distribution:**
   \[ P(X = \bm{k}; n, \bm{p}) = \frac{n!}{k_1! \cdot k_2! \cdot \ldots \cdot k_r!} p_1^{k_1} p_2^{k_2} \ldots p_r^{k_r} \]

9. **Negative Binomial Distribution:**
   \[ P(X = k; r, p) = \binom{k+r-1}{r-1} (1-p)^r p^k \]

\end{comment}










































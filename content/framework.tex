
%\texttt{make\_classification}
\section{Framework}

The effectiveness of an inference procedure depends on many factors and components, 
and with so many variables in play general conclusions are difficult to obtain from empirical tests.
Several models and settings were tested in this project, but the results obtained may not translate to other circumstances.

So while there is value in conducting these experiments and conclusions can be drawn from them, 
providing the infrastructure to conduct specialised experiments in the future might be a more fruitful contribution.
Thus a central goal of this project was to create a code framework, 
that facilitates setting up and combining components in a way that can be tailored to circumstances of interest which are not discussed here.

Designing code for generality is a difficult task but the created modules should allow tests of the RL and ML estimation techniques for a variety of energy based models.
In the design special care was taken to keep the components modular and extensible wherever possible, 
so hopefully the framework can not only be used to reproduce the results discussed here but serve as a basis for further investigation.
Another key objective in the design was maximising computational efficiency in the major components.
Thus all of the core components operate purely on \texttt{torch.Tensor}s and allow for leveraging the optimised \texttt{torch} GPU processing.


\subsection{The Energy Model}

The estimation process consists of calculating the respective likelihood gradients that a \texttt{torch} optimiser then uses to update the model parameters during training.
These gradients are obtained as Monte Carlo estimates of the expectations show in chapter ( ).

For the model part samples from the model distribution need to be generated and their parameter gradients evaluated batch-wise.
Here the model samples are produced via MCMC samplers whose iterations require evaluating the energy functional and the gradient with respect to the input of that functional.
Thus the model class needed methods for calculating the energy functional $U_{\bm{\theta}} (\bm{x})$, 
the gradient with respect to the input $\nabla_{\bm{x}} U_{\bm{\theta}} (\bm{x})$ 
and the gradient with respect to the parameters evaluated at the input $\nabla_{\bm{\theta}} U_{\bm{\theta}} (\bm{x})$.

This is achieved by the \texttt{EnergyModel} base class, which inherits from \texttt{torch.nn.Module}, 
to integrate well with the machine learning architecture \texttt{torch} provides.
It initialises with an empty \texttt{params} dictionary of type \texttt{torch.nn.ParameterDict} and has the methods
\begin{enumerate}
	\item \texttt{forward(x)} which relays calls to \texttt{energy}
	\item \texttt{energy(x)} corresponding to $U_{\bm{\theta}} (\bm{x})$
	\item \texttt{energy\_grad(x)} corresponding to $\nabla_{\bm{x}} U_{\bm{\theta}} (\bm{x})$
	\item \texttt{avg\_param\_grad(x)} which directly calculates $\frac{1}{n} \sum_{j=1}^n \nabla_{\bm{\theta}} U_{\bm{\theta}} (\bm{x}_j)$
\end{enumerate}

where \texttt{x} can be either a single input or a batch of inputs.
The method \texttt{avg\_param\_grad} stands out here as for a batch it does not produce a matrix of gradients, but already averages the gradients across samples,
effectively calculating the MC estimate for $\mathbb{E}_{ \bm{x} \sim p_{\bm{\theta}}} \left[ \frac{\dif}{\dif \bm{\theta}} U_{\bm{\theta}} (\bm{x}) \right]$ directly.

Any desired energy model, subclassing \texttt{EnergyModel}, needs only inscribe its named parameters in the \texttt{params} dictionary and override the \texttt{energy} method.
The \texttt{energy\_grad} and \texttt{avg\_param\_grad} methods can be overridden, if an analytical representation of these gradients is available, 
or inherited from the base class, where they leverage \texttt{torch}s automatic differentiation capability.
The sampler and ML classes can use any instance of such a model directly. 


\subsection{Recovery Adapter}

For RL estimation it is more complicated as the gradients are taken with respect to the conditional energy that depends on the perturbed samples 
and requires extra terms, that are derived from the normal distribution of the noise.
One approach could have been to create parallel class hierarchies and create a regular and a conditional class for every model, training and evaluating them separately.
This would have made the pipeline and evaluation more complicated however, and since the conditional version is only relevant during training, 
there would have been a lot of boilerplate.

The chosen solution for this problem is utilising the Object Adapter design pattern with the \texttt{RecoveryAdapter} class.
Essentially the adapter both inherits from \texttt{EnergyModel} and incorporates an instance of this type at the same time,
overriding the methods by relaying the calls first to this instance and then adding the conditional terms.
It has one extra method that allows to set a batch of perturbed samples which are utilised in these conditional terms.
At every training iteration a perturbed version of the current batch of training data is created by adding Gaussian noise in the RecoveryLikelihood class 
and passing them to the adapted model before generating samples and calculating the parameter gradient.
The major benefits of this approach are that, for one any model can be adapted directly, without having to design an additional class,
and that after training the actual model can simply be retrieved from within the adapter and the adapter can be discarded without the need for transformation.




























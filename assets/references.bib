%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Manuel Gnthr at 2023-11-30 16:52:38 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@article{DBLP:journals/corr/abs-2012-08125,
	author = {Ruiqi Gao and Yang Song and Ben Poole and Ying Nian Wu and Diederik P. Kingma},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2012-08125.bib},
	date-added = {2023-11-30 16:52:31 +0100},
	date-modified = {2023-11-30 16:52:31 +0100},
	eprint = {2012.08125},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Sat, 02 Jan 2021 15:43:30 +0100},
	title = {Learning Energy-Based Models by Diffusion Recovery Likelihood},
	url = {https://arxiv.org/abs/2012.08125},
	volume = {abs/2012.08125},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2012.08125}}

@inproceedings{10.5555/3104482.3104568,
	abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an inbuilt protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a "sampling threshold" and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
	address = {Madison, WI, USA},
	author = {Welling, Max and Teh, Yee Whye},
	booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
	date-added = {2023-11-30 16:50:18 +0100},
	date-modified = {2023-11-30 16:50:18 +0100},
	isbn = {9781450306195},
	location = {Bellevue, Washington, USA},
	numpages = {8},
	pages = {681--688},
	publisher = {Omnipress},
	series = {ICML'11},
	title = {Bayesian Learning via Stochastic Gradient Langevin Dynamics},
	year = {2011}}

@article{JMLR:v6:hyvarinen05a,
	author = {Aapo Hyv{{\"a}}rinen},
	date-added = {2023-11-30 16:47:36 +0100},
	date-modified = {2023-11-30 16:47:36 +0100},
	journal = {Journal of Machine Learning Research},
	number = {24},
	pages = {695--709},
	title = {Estimation of Non-Normalized Statistical Models by Score Matching},
	url = {http://jmlr.org/papers/v6/hyvarinen05a.html},
	volume = {6},
	year = {2005},
	bdsk-url-1 = {http://jmlr.org/papers/v6/hyvarinen05a.html}}

@article{DBLP:journals/corr/abs-1912-03263,
	author = {Will Grathwohl and Kuan{-}Chieh Wang and J{\"{o}}rn{-}Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1912-03263.bib},
	date-added = {2023-11-30 16:45:59 +0100},
	date-modified = {2023-11-30 16:45:59 +0100},
	eprint = {1912.03263},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Tue, 07 Jan 2020 13:36:12 +0100},
	title = {Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One},
	url = {http://arxiv.org/abs/1912.03263},
	volume = {abs/1912.03263},
	year = {2019},
	bdsk-url-1 = {http://arxiv.org/abs/1912.03263}}
